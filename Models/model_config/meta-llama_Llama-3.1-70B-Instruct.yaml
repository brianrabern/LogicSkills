model:
  init:
    torch_dtype: "bfloat16"
  inference:
    max_new_tokens: 2500
    do_sample: False
tokenizer:
  padding_side: "left"
prompt:
  system_message: True