model:
  init:
    torch_dtype: "auto"
  inference:
    max_new_tokens: 7500
    temperature: 0.6
    top_p: 0.95
    top_k: 20
    min_p: 0
tokenizer:
  padding_side: "left"
prompt:
  system_message: True